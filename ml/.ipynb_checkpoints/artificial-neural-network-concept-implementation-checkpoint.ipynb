{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Churn_Modelling.csv']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f7dd9cc4ee816fc0607916ccf3f9e7089ffbe9d7"
   },
   "source": [
    "## Artificial Neural networks\n",
    "Artificial Neural networks (ANN) or neural networks are computational algorithms.\n",
    "A neural network is a machine learning algorithm based on the model of a human neuron. The human brain consists of millions of neurons. It sends and process signals in the form of electrical and chemical signals. These neurons are connected with a special structure known as synapses. Synapses allow neurons to pass signals. From large numbers of simulated neurons neural networks forms.\n",
    "An Artificial Neural Network is an information processing technique. It works like the way human brain processes information. ANN includes a large number of connected processing units that work together to process information. They also generate meaningful results from it.\n",
    "We can apply Neural network not only for classification. It can also apply for regression of continuous target attributes.\n",
    "Neural networks find great application in data mining used in sectors. For example economics, forensics, etc and for pattern recognition. It can be also used for data classification in a large amount of data after careful training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f2eec1f3c924547d66b2619ec3123b07927e1e77"
   },
   "source": [
    "## How ANN Works\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/0*0mia7BQKjUAuXeqZ.jpeg\" width=\"900\"></img>\n",
    "### Look carefully at the diagram given above.Information (data) is travelling or going from input layer to hidden layers and then to output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "25d587e6e65c4590ff97620e282a3a6475dfdef5"
   },
   "source": [
    "## Three types of layers in Artificial Neural network\n",
    "\n",
    "Artificial Neural network is typically organized in layers. Layers are being made up of many interconnected ‘nodes’ which contain an ‘activation function’. A neural network may contain the following 3 layers:\n",
    "\n",
    "### a. Input layer\n",
    "The purpose of the input layer is to receive as input the values of the explanatory attributes for each observation. Usually, the number of input nodes in an input layer is equal to the number of explanatory variables. ‘input layer’ presents the patterns to the network, which communicates to one or more ‘hidden layers’.\n",
    "The nodes of the input layer are passive, meaning they do not change the data. They receive a single value on their input and duplicate the value to their many outputs. From the input layer, it duplicates each value and sent to all the hidden nodes.\n",
    "\n",
    "### b. Hidden layer\n",
    "The Hidden layers apply given transformations to the input values inside the network. In this, incoming arcs that go from other hidden nodes or from input nodes connected to each node. It connects with outgoing arcs to output nodes or to other hidden nodes. In hidden layer, the actual processing is done via a system of weighted ‘connections’. There may be one or more hidden layers. The values entering a hidden node multiplied by weights, a set of predetermined numbers stored in the program. The weighted inputs are then added to produce a single number.\n",
    "\n",
    "### c. Output layer\n",
    "The hidden layers then link to an ‘output layer‘. Output layer receives connections from hidden layers or from input layer. It returns an output value that corresponds to the prediction of the response variable. In classification problems, there is usually only one output node. The active nodes of the output layer combine and change the data to produce the output values.\n",
    "\n",
    "The ability of the neural network to provide useful data manipulation lies in the proper selection of the weights. This is different from conventional information processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "466a710a4d298df969f3db2fdb9a83a291326f34"
   },
   "source": [
    "<h1>Activation Function</h1>\n",
    "Activation functions are really important for a Artificial Neural Network to learn and make sense of something really complicated and Non-linear complex functional mappings between the inputs and response variable.They introduce non-linear properties to our Network.Their main purpose is to convert a input signal of a node in a A-NN to an output signal. That output signal now is used as a input in the next layer in the stack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "99f848558e4a630b8b9913b6f4b4fd9f61414465"
   },
   "source": [
    "<h1>Most popular types of Activation functions<h1>\n",
    "<h3>1.Sigmoid or Logistic</h3>\n",
    "<h3>2.Tanh — Hyperbolic tangent</h3>\n",
    "<h3>3.ReLu -Rectified linear unit</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1ddd2b9526aa3561462eca3c8be4cdfd412e4101"
   },
   "source": [
    "for more study about Activation function visit this link\n",
    "https://towardsdatascience.com/activation-functions-and-its-types-which-is-better-a9a5310cc8f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4a587896b172cf982b0b4a3b1ba0223995d4222e"
   },
   "source": [
    "<h1>Backpropogation</h1>\n",
    "The backpropagation algorithm was originally introduced in the 1970s, but its importance wasn't fully appreciated until a famous 1986 paper by David Rumelhart, Geoffrey Hinton, and Ronald Williams. That paper describes several neural networks where backpropagation works far faster than earlier approaches to learning, making it possible to use neural nets to solve problems which had previously been insoluble. Today, the backpropagation algorithm is the workhorse of learning in neural networks.\n",
    "\n",
    "At the heart of backpropagation is an expression for the partial derivative ∂C/∂w of the cost function C with respect to any weight w (or bias b) in the network. The expression tells us how quickly the cost changes when we change the weights and biases. And while the expression is somewhat complex, it also has a beauty to it, with each element having a natural, intuitive interpretation. And so backpropagation isn't just a fast algorithm for learning. It actually gives us detailed insights into how changing the weights and biases changes the overall behaviour of the network. That's well worth studying in detail.\n",
    "\n",
    "Let's begin with a notation which lets us refer to weights in the network in an unambiguous way. We'll use wljk to denote the weight for the connection from the **kth** neuron in the** (l−1)th** layer to the** jth **neuron in the** lth** layer. So, for example, the diagram below shows the weight on a connection from the fourth neuron in the second layer to the second neuron in the third layer of a network: \n",
    "<img src=\"http://neuralnetworksanddeeplearning.com/images/tikz16.png\" width=\"500\"></img>\n",
    "\n",
    "This notation is cumbersome at first, and it does take some work to master. But with a little effort you'll find the notation becomes easy and natural. One quirk of the notation is the ordering of the** j **and** k** indices. You might think that it makes more sense to use **j **to refer to the input neuron, and** k **to the output neuron, not vice versa, as is actually done. I'll explain the reason for this quirk below.\n",
    "\n",
    "We use a similar notation for the network's biases and activations. Explicitly, we use blj\n",
    "for the bias of the **jth neuron in the lth **layer. And we use alj for the activation of the **jth neuron in the lth** layer. The following diagram shows examples of these notations in use: \n",
    "<img src=\"http://neuralnetworksanddeeplearning.com/images/tikz17.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "da9b2a75a76adb24f2a9e409f0ed7e8362ba227a"
   },
   "source": [
    "With these notations, the activation** alj of the jth neuron in the lth layer is related to the activations in the (l−1)th** layer by the equation \n",
    "<h1>**alj=σ(∑kwljkal−1k+blj)**<h1>\n",
    "\n",
    "**where the sum is over all neurons k in the (l−1)th layer. To rewrite this expression in a matrix form we define a weight matrix wl for each layer, l. The entries of the weight matrix wl are just the weights connecting to the lth layer of neurons, that is, the entry in the jth row and kth column is wljk. Similarly, for each layer l we define a bias vector, bl. You can probably guess how this works - the components of the bias vector are just the values blj, one component for each neuron in the lth layer. And finally, we define an activation vector al whose components are the activations alj**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "df4ae03d8a946885eaf7c4e94647503cea7a626d"
   },
   "source": [
    "**<h1> Implemantation </h1>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "c42315e7112d2062e93adbd7914e45c118dc7131"
   },
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "a51bdfbbc8edd691cfae2ecd5dd9706854fdb5a0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   ...   EstimatedSalary  Exited\n",
       "0          1    15634602   ...         101348.88       1\n",
       "1          2    15647311   ...         112542.58       0\n",
       "2          3    15619304   ...         113931.57       1\n",
       "3          4    15701354   ...          93826.63       0\n",
       "4          5    15737888   ...          79084.10       0\n",
       "\n",
       "[5 rows x 14 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"../input/Churn_Modelling.csv\",encoding = \"ISO-8859-1\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "028eb2152e4d2aacdd0b132afa5a653981e45321"
   },
   "source": [
    "## Task Overview\n",
    "this dataset is the data of a bank, who want to detect which are more likely to live the bank and which customer will set in bank.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5fcfb4284e9d64f1169840a66764b337e5da2688"
   },
   "source": [
    "Remove columns which are not used full for us.\n",
    "remove first three columns surname, Rownumber, CustoomerId these are the unnecessary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "624023def0357b8bc11ecc01a8cbae5d288443f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[619, 'France', 'Female', ..., 1, 1, 101348.88],\n",
       "       [608, 'Spain', 'Female', ..., 0, 1, 112542.58],\n",
       "       [502, 'France', 'Female', ..., 1, 0, 113931.57],\n",
       "       ...,\n",
       "       [709, 'France', 'Female', ..., 0, 1, 42085.58],\n",
       "       [772, 'Germany', 'Male', ..., 1, 0, 92888.52],\n",
       "       [792, 'France', 'Female', ..., 1, 0, 38190.78]], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Remove columns \n",
    "X = dataset.iloc[:, 3:13].values\n",
    "y = dataset.iloc[:, 13].values\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3252926966f1fc5f03eb66ccd220e0b2396122a4"
   },
   "source": [
    "Deal with categorical data \n",
    "Columns \"Geography\" and Gender Contain categorical data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "f36450578d6abebacafca23da8eb4f18ca496a57"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:390: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Encoding categorical data\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\n",
    "labelencoder_X_2 = LabelEncoder()\n",
    "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [1])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "X = X[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "b7b3252dc8c9e7f8f492737b73b5190462571754"
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "6b9bdc33c1843d42fef271f95de1cef5e91a81af"
   },
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "039bcf99dd75d672aebd33dec2f1819738852075"
   },
   "source": [
    "### After data preprocessing it's time for build ANN(Artificial Neaural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "aae45f12de975fce4130711f3d465d409a73e521"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "730b37aa6880ad59a66deffea8007ab51875dea1"
   },
   "source": [
    "### Keras is a minimalist Python library for deep learning that can run on top of Theano or TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "7288f02615c8216e3367bee045d150fc8e2a5c85"
   },
   "outputs": [],
   "source": [
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "44d880e9fe8ab5b7af43a4c6fc68b95388cede8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 1s 161us/step - loss: 0.4896 - acc: 0.7955\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 0.4307 - acc: 0.7960\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 0.4261 - acc: 0.7960\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 0.4214 - acc: 0.8045\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 0.4179 - acc: 0.8242\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 0.4157 - acc: 0.8272\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 0.4139 - acc: 0.8292\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 0.4126 - acc: 0.8310\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 0.4114 - acc: 0.8326\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 0.4107 - acc: 0.8322\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 0.4096 - acc: 0.8354\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 0.4088 - acc: 0.8341\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 0.4077 - acc: 0.8341\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 0.4070 - acc: 0.8340\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 0.4068 - acc: 0.8344\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 0.4062 - acc: 0.8345\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 0.4054 - acc: 0.8342\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 0.4051 - acc: 0.8347\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 0.4049 - acc: 0.8349\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 0.4041 - acc: 0.8344\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 0.4043 - acc: 0.8345\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 0.4039 - acc: 0.8349\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 0.4036 - acc: 0.8345\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 0.4031 - acc: 0.8352\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 0.4031 - acc: 0.8354\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 0.4026 - acc: 0.8339\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 0.4028 - acc: 0.8351\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 0.4029 - acc: 0.8351\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 0.4028 - acc: 0.8347\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 0.4022 - acc: 0.8357\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 0.4019 - acc: 0.8356\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 0.4020 - acc: 0.8374\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 0.4018 - acc: 0.8336\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 0.4016 - acc: 0.8347\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 0.4014 - acc: 0.8356\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 0.4014 - acc: 0.8347\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 0.4012 - acc: 0.8351\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 0.4013 - acc: 0.8354\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 0.4008 - acc: 0.8356\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 0.4008 - acc: 0.8355\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 0.4009 - acc: 0.8355\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 0.4006 - acc: 0.8369\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 0.4011 - acc: 0.8352\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 0.4008 - acc: 0.8351\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 0.4008 - acc: 0.8349\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 0.4005 - acc: 0.8354\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 0.4003 - acc: 0.8339\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 0.4004 - acc: 0.8362\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 0.4005 - acc: 0.8350\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 0.4001 - acc: 0.8351\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 0.4005 - acc: 0.8350\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 0.4001 - acc: 0.8342\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 0.4008 - acc: 0.8356\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 0.4000 - acc: 0.8350\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 0.4003 - acc: 0.8359\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 0.4000 - acc: 0.8360\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 0.3999 - acc: 0.8345\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 0.4005 - acc: 0.8354\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 0.3999 - acc: 0.8341\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 0.4001 - acc: 0.8356\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 0.4000 - acc: 0.8344\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 0.3996 - acc: 0.8362\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 0.4002 - acc: 0.8340\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 0.3999 - acc: 0.8351\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 0.3998 - acc: 0.8357\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 0.4000 - acc: 0.8350\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 0.3997 - acc: 0.8354\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 0.3996 - acc: 0.8344\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 0.3999 - acc: 0.8346\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 0.3991 - acc: 0.8362\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 0.4000 - acc: 0.8366\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 0.3997 - acc: 0.8355\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 0.4001 - acc: 0.8340\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 0.3995 - acc: 0.8346\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 0.3993 - acc: 0.8364\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 0.3994 - acc: 0.8350\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 0.4001 - acc: 0.8359\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 0.3998 - acc: 0.8351\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 0.3998 - acc: 0.8350\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 0.4001 - acc: 0.8349\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 0.3989 - acc: 0.8366\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 0.3997 - acc: 0.8337\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 0.3999 - acc: 0.8350\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 0.3994 - acc: 0.8355\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 0.3994 - acc: 0.8346\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 0.3995 - acc: 0.8356\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 0.3996 - acc: 0.8359\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 0.3996 - acc: 0.8332\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 0.3999 - acc: 0.8345\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 0.3991 - acc: 0.8354\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 0.3996 - acc: 0.8359\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 0.3994 - acc: 0.8360\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 0.3997 - acc: 0.8342\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 0.3999 - acc: 0.8362\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 0.3993 - acc: 0.8357\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 0.3991 - acc: 0.8367\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 0.3997 - acc: 0.8359\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 0.3994 - acc: 0.8345\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 0.3995 - acc: 0.8357\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 0.3995 - acc: 0.8354\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0c34ca3e80>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(X_train, y_train, batch_size = 10, epochs = 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "4cc689277de897733f64b6d81fe69690e9b41cd7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False],\n",
       "       [False],\n",
       "       [False],\n",
       "       ...,\n",
       "       [False],\n",
       "       [False],\n",
       "       [False]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "318149d5c41ab4c36f4dbceb06786dea8f8a7084"
   },
   "outputs": [],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "84938a9ab91dfcd13c7cccc7bba2bb1d7f1bfde5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1536,   59],\n",
       "       [ 260,  145]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "2dcbd9617cdf284b571b738baee345bb6f5f035f"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
